{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X7ZwNViRWmT"
      },
      "source": [
        "# CC5215: Privacidad de Datos\n",
        "\n",
        "## Laboratorio 10\n",
        "\n",
        "Integrantes:\n",
        "\n",
        "- Nombre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JmjcAJRRWmW"
      },
      "outputs": [],
      "source": [
        "# Load the data and libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def laplace_mech(v, sensitivity, epsilon):\n",
        "    return v + np.random.laplace(loc=0, scale=sensitivity / epsilon)\n",
        "\n",
        "def gaussian_mech_vec(vec, sensitivity, epsilon, delta):\n",
        "    return [v + np.random.normal(loc=0, scale=sensitivity * np.sqrt(2*np.log(1.25/delta)) / epsilon)\n",
        "            for v in vec]\n",
        "\n",
        "def pct_error(orig, priv):\n",
        "    return np.abs(orig - priv)/orig * 100.0\n",
        "\n",
        "adult = pd.read_csv('https://users.dcc.uchile.cl/~mtoro/cursos/cc5215/adult_with_pii.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preamble\n",
        "\n",
        "In this lab we are going to experiment with more advanced variants of differential privacy. These advanced variants are based on specific statistics concepts and as we will see, they will allow us to compose private results with tighter bounds. Finally, we will implement the noisy gradient descent algorithm seen in class, but with different variants of DP."
      ],
      "metadata": {
        "id": "2DN9AAekEaWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (4 points)\n",
        "\n",
        "Look at the two graphs:\n",
        "\n",
        "- What information is presented in each plot?\n",
        "- What can you say about sequential composition for $\\epsilon$-DP? what about $(\\epsilon, \\delta)$-DP?"
      ],
      "metadata": {
        "id": "CZVUEi23puAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ks = np.linspace(1, 100, 20)\n",
        "\n",
        "\n",
        "## (epsilon)\n",
        "\n",
        "epsilon = .1\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "# L1 sensitivity of each query: 1\n",
        "# noise per query: 1/epsilon\n",
        "# number of queries: k\n",
        "noises_seq_eps = [k*(1/epsilon) for k in ks]\n",
        "plt.plot(ks, noises_seq_eps, label='Sequential Composition')\n",
        "\n",
        "# number of queries: 1\n",
        "# L1 sensitivity of each query: k\n",
        "# noise per query: k / epsilon\n",
        "noises_L1_eps = [1*(k/epsilon) for k in ks]\n",
        "plt.plot(ks, noises_L1_eps, label='Vectorized')\n",
        "plt.title('Laplace Mechanism: Vectorized vs. Composition', {\n",
        "    'fontsize': 8\n",
        "})\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Scale of Noise')\n",
        "plt.legend();\n",
        "\n",
        "## (epsilon, delta)\n",
        "\n",
        "epsilon = .1\n",
        "delta = 1e-5\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# L2 sensitivity of each query: 1\n",
        "# number of queries: k\n",
        "noises_seq_epsdel = [16*k*np.log((2.5*k)/delta)*np.log(2/delta)/(epsilon**2) for k in ks]\n",
        "plt.plot(ks, noises_seq_epsdel, label='Advanced Composition')\n",
        "\n",
        "# number of queries: 1\n",
        "# L2 sensitivity of each query: sqrt(k)\n",
        "noises_L1_epsdel = [2*k*np.log(1.25/delta)/(epsilon**2) for k in ks]\n",
        "plt.title('Gaussian Mechanism: Vectorized vs. Composition', {\n",
        "    'fontsize': 8,\n",
        "})\n",
        "plt.xlabel('Number of Queries')\n",
        "plt.ylabel('Scale of Noise')\n",
        "plt.plot(ks, noises_L1_epsdel, label='Vectorized')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "00OHQ9fsqK1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- YOUR ANSWER HERE\n",
        "- YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "8Q5x_7x_rTo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving sequential composition\n",
        "\n",
        "> Wouldn't you like a better sequential composition? ğŸ¤”\n",
        "\n",
        "In statistics, a [divergence](https://en.wikipedia.org/wiki/Divergence_(statistics)) is a way of measuring the distance between two probability distributions - which is exactly what we want to do for differential privacy. The *max divergence* between two probability distributions $Y$ and $Z$ is defined to be:\n",
        "\n",
        "$$\n",
        "D_\\infty(Y, Z) = \\max_{S \\subseteq \\mathsf{Supp}(Y)}\\Big(\\log \\frac{Pr[Y \\in S]}{Pr[Z \\in S]}\\Big)\n",
        "$$\n",
        "\n",
        "One nice thing about max divergence is that the definition of differential privacy can be easily recovered. In fact, $F$ satisfies $\\epsilon$-DP if:\n",
        "\n",
        "$$\n",
        "D_\\infty(F(x), F(x')) \\leq \\epsilon\n",
        "$$\n",
        "\n",
        "Another interesting definition of a divergence is the [RÃ©nyi divergence](https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#R%C3%A9nyi_divergence) which, like max divergence, allows us to recover the definition of differential privacy. The RÃ©nyi divergence of order $\\alpha$ between probability distributions $P$ and $Q$ is defined as (where $P(x)$ and $Q(x)$ denote the probability density of $P$ and $Q$ at point $x$, respectively):\n",
        "\n",
        "$$\n",
        "D_\\alpha(P, Q) = \\frac{1}{\\alpha - 1}\\log E_{x \\sim Q}\\Big(\\frac{P(x)}{Q(x)}\\Big)^\\alpha\n",
        "$$\n",
        "\n",
        "If we set $\\alpha = \\infty$, we immediately recover the definition of $\\epsilon$-DP. But what if we set $\\alpha$ to something else?\n",
        "\n",
        "In 2017, Ilya Mironov proposed [RÃ©nyi differential privacy (RDP)](https://arxiv.org/abs/1702.07476). A randomized mechanism $F$ satisfies $(\\alpha, \\epsilon_r)$-RDP if, for all neighboring datasets $x$ and $x'$: $^1$\n",
        "\n",
        "$$\n",
        "D_\\alpha(F(x), F(x')) \\leq \\epsilon_r\n",
        "$$\n",
        "\n",
        "A key property of RÃ©nyi differential privacy is that a mechanism which satisfies RDP also satisfies $(\\epsilon, \\delta)$-differential privacy. Specifically, if $F$ satisfies $(\\alpha, \\epsilon_r)$-RDP, then for $\\delta > 0$, $F$ satisfies $(\\epsilon, \\delta)$-differential privacy for $\\epsilon = \\epsilon_r + \\frac{\\log(1 / \\delta)}{\\alpha - 1}$. The analyst is free to pick any value of $\\delta$; a meaningful value (e.g. $\\delta \\leq \\frac{1}{n^2}$) should be picked in practice.\n",
        "\n",
        "A basic mechanism for achieving RDP is the Gaussian mechanism. Specifically, for a function $f : \\mathcal{D} \\to \\mathbb{R}^k$ with $L2$ sensitivity of $\\Delta f$, the following mechanism satisfies $(\\alpha, \\epsilon_r)$-RDP:\n",
        "\n",
        "$$\n",
        "F(x) = f(x) + \\mathcal{N}(\\sigma^2)\\text{ where }\\sigma^2 = \\frac{\\Delta f^2 \\alpha}{2\\epsilon_r}\n",
        "$$\n",
        "\n",
        "$^1$: We use $\\epsilon_r$ to differentiate it from the parameter of regular $\\epsilon$-DP."
      ],
      "metadata": {
        "id": "U2ryn2S8sHnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (4 points)\n",
        "\n",
        "Implement the gaussian mechanism for RÃ©nyi DP:\n",
        "\n",
        "*Hint*: Use `np.random.normal(scale=std, size=n)` with `std` as the standard deviation and `n` the number of sampless to generate."
      ],
      "metadata": {
        "id": "HRw_IRuuj1rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_mech_RDP_vec(vec, sensitivity, alpha, epsilon_r):\n",
        "    '''Adds gaussian noice to the vector considering RDP.\n",
        "\n",
        "    Args:\n",
        "      vec: numpy array of values to which noise will be added.\n",
        "      sensitivity: L2 sensitivity of the query.\n",
        "      alpha: RDP parameter.\n",
        "      epsilon_r: privacy parameter for RDP.\n",
        "\n",
        "    Returns:\n",
        "      The numpy array with the added Gaussian noise.\n",
        "    '''\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "2lImH0YK1GRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing noisy gradient descent with RÃ©nyi\n",
        "\n",
        "The following code blocks include utility functions and definitions:"
      ],
      "metadata": {
        "id": "GHIzN2yjpsT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data files\n",
        "import urllib.request\n",
        "import io\n",
        "\n",
        "url_x = 'https://users.dcc.uchile.cl/~mtoro/cursos/cc5215/adult_processed_x.npy'\n",
        "url_y = 'https://users.dcc.uchile.cl/~mtoro/cursos/cc5215/adult_processed_y.npy'\n",
        "\n",
        "with urllib.request.urlopen(url_x) as url:\n",
        "    f = io.BytesIO(url.read())\n",
        "X = np.load(f)\n",
        "\n",
        "with urllib.request.urlopen(url_y) as url:\n",
        "    f = io.BytesIO(url.read())\n",
        "y = np.load(f)"
      ],
      "metadata": {
        "id": "pV_uFgboohTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "training_size = int(X.shape[0] * 0.8)\n",
        "\n",
        "X_train = X[:training_size]\n",
        "X_test = X[training_size:]\n",
        "\n",
        "y_train = y[:training_size]\n",
        "y_test = y[training_size:]\n",
        "\n",
        "print('Train and test set sizes:', len(y_train), len(y_test))"
      ],
      "metadata": {
        "id": "5YtACvI4hfXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "## Functions seen in class\n",
        "##\n",
        "\n",
        "# The loss function measures how good our model is. The training goal is to minimize the loss.\n",
        "# This is the logistic loss function.\n",
        "def loss(theta, xi, yi):\n",
        "    exponent = - yi * (xi.dot(theta))\n",
        "    return np.log(1 + np.exp(exponent))\n",
        "\n",
        "# This is the gradient of the logistic loss\n",
        "# The gradient is a vector that indicates the rate of change of the loss in each direction\n",
        "def gradient(theta, xi, yi):\n",
        "    exponent = yi * (xi.dot(theta))\n",
        "    return - (yi*xi) / (1+np.exp(exponent))\n",
        "\n",
        "def avg_grad(theta, X, y):\n",
        "    grads = [gradient(theta, xi, yi) for xi, yi in zip(X, y)]\n",
        "    return np.mean(grads, axis=0)\n",
        "\n",
        "# Prediction: take a model (theta) and a single example (xi) and return its predicted label\n",
        "def predict(xi, theta, bias=0):\n",
        "    label = np.sign(xi @ theta + bias)\n",
        "    return label\n",
        "\n",
        "def accuracy(theta):\n",
        "    return np.sum(predict(X_test, theta) == y_test)/X_test.shape[0]\n",
        "\n",
        "# L2 Clipping\n",
        "def L2_clip(v, b):\n",
        "    norm = np.linalg.norm(v, ord=2)\n",
        "\n",
        "    if norm > b:\n",
        "        return b * (v / norm)\n",
        "    else:\n",
        "        return v\n",
        "\n",
        "def gradient_sum(theta, X, y, b):\n",
        "    gradients = [L2_clip(gradient(theta, x_i, y_i), b) for x_i, y_i in zip(X,y)]\n",
        "\n",
        "    # sum query\n",
        "    # L2 sensitivity is b (by clipping performed above)\n",
        "    return np.sum(gradients, axis=0)\n"
      ],
      "metadata": {
        "id": "X2C_wF_Oho99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Composition of RDP\n",
        "\n",
        "If\n",
        "- $F_1$ satisfies $(\\alpha, \\epsilon_{r1})$-RDP, and\n",
        "- $F_2$ satisfies $(\\alpha, \\epsilon_{r2})$-RDP.\n",
        "- Then, their composition satisfies $(\\alpha, \\epsilon_{r1} + \\epsilon_{r2})$-RDP."
      ],
      "metadata": {
        "id": "3CgZaXbHj7U1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 (10 points)\n",
        "\n",
        "Implement a version of noisy gradient that uses RÃ©nyi Differential Privacy. The total privacy cost of `noisy_gradient_descent_RDP` must be of $(\\alpha, \\epsilon_r)$."
      ],
      "metadata": {
        "id": "8OTj6-f8gImM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noisy_gradient_descent_RDP(iterations, alpha, epsilon_r):\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "M9beOBT_3CPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = accuracy(noisy_gradient_descent_RDP(10, 100, 1))\n",
        "assert (test_acc > .7) and (test_acc < .9)"
      ],
      "metadata": {
        "id": "bTQ1ttt1HZd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (6 points)\n",
        "\n",
        "- Argument, in terms of the equivalence between RÃ©nyi-DP and $(\\epsilon, \\delta)$-DP, how an $\\epsilon$-DP laplace mechanism can be sequentially combined with a RÃ©nyi-DP guassian mechanism.\n",
        "- Explain why the total cost of your implementation of `noisy_gradient_descent_RDP` is $(\\alpha, \\epsilon_r)$-RDP."
      ],
      "metadata": {
        "id": "yIQH87yFgdK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing noisy gradient descent with Zero-Concentrated Differential Privacy\n",
        "\n",
        "In concurrent work to Ilya, in 2016, Mark Bun and Thomas Steinke proposed [zero-concentrated differential privacy (zCDP)](https://arxiv.org/abs/1605.02065). Similar to RDP, zCDP is defined in terms of the RÃ©nyi divergence, but it includes only a single privacy parameter ($\\rho$). A randomized mechanism $F$ satisfies $\\rho$-zCDP if for all neighboring datasets $x$ and $x'$, and all $\\alpha \\in (0, \\infty)$:\n",
        "\n",
        "$$\n",
        "D_\\alpha(F(x), F(x')) \\leq \\rho \\alpha\n",
        "$$\n",
        "\n",
        "This presents a stronger guarantee than RDP, because it restricts RÃ©nyi divergence in many orders. However, the bound becomes more relaxed as $\\alpha$ grows.\n",
        "\n",
        "Zero-concentrated DP can also be converted to $(\\epsilon, \\delta)$-DP: if $F$ satisfies $\\rho$-zCDP, then for any $\\delta > 0$, $F$ satisfies $(\\epsilon, \\delta)$-DP for $\\epsilon = \\rho + 2 \\sqrt{\\rho \\log(1 / \\delta)}$.\n",
        "\n",
        "zCDP is also similar to RDP in that the Gaussian mechanism can be used as a basic mechanism. Specifically, for a function $f : \\mathcal{D} \\to \\mathbb{R}^k$ with $L2$ sensitivity of $\\Delta f$, the following mechanism satisfies $(\\alpha, \\epsilon_r)$-RDP:\n",
        "\n",
        "$$\n",
        "F(x) = f(x) + \\mathcal{N}(\\sigma^2)\\text{ where }\\sigma^2 = \\frac{\\Delta f^2}{2\\rho}\n",
        "$$"
      ],
      "metadata": {
        "id": "o7ZULX6rhNr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 (2 points)\n",
        "\n",
        "Implement the guassian mechanism for zCDP:"
      ],
      "metadata": {
        "id": "JioRIiKFjgVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_mech_zCDP_vec(vec, sensitivity, rho):\n",
        "    '''Adds gaussian noice to the vector considering zCDP.\n",
        "\n",
        "    Args:\n",
        "      vec: numpy array of values to which noise will be added.\n",
        "      sensitivity: L2 sensitivity of the query.\n",
        "      rho: privacy parameter for zCDP.\n",
        "\n",
        "    Returns:\n",
        "        The numpy array with the added Gaussian noise.\n",
        "    '''\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "G2AwcDCBjnqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Composition of zCDP\n",
        "\n",
        "If\n",
        "\n",
        "- $F_1$ satisfies $\\rho_1$-zCDP, and\n",
        "- $F_2$ satisfies $\\rho_2$-zCDP.\n",
        "- Then, their composition satisfies $(\\rho_1 + \\rho_2)$-zCDP"
      ],
      "metadata": {
        "id": "wks8xELLkjvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6 (6 points)\n",
        "\n",
        "Implement a version of noisy gradient descent using zCDP. Your solution should have a total cost of $\\rho$-zCDP."
      ],
      "metadata": {
        "id": "dWTaAY1Ik49P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noisy_gradient_descent_zCDP(iterations, rho):\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "iBbMbsuwk4K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = accuracy(noisy_gradient_descent_zCDP(10, 1))\n",
        "assert (test_acc > .7) and (test_acc < .9)"
      ],
      "metadata": {
        "id": "E7r0bQLrHVUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7 (8 points)\n",
        "\n",
        "- Argument, in terms of the equivalence between zCDP and $(\\epsilon, \\delta)$-DP, how an $\\epsilon$-DP laplace mechanism can be sequentially combined with a $\\rho$-zCDP guassian mechanism. What is different with respect to your previous reasoning when working with RDP?\n",
        "- Explain why the total cost of your implementation of `noisy_gradient_descent_zCDP` is $\\rho$-RDP."
      ],
      "metadata": {
        "id": "KBNP28FFpQXa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- YOUR ANSWER HERE\n",
        "- YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "ooYIyJFwqZf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8 (12 points)\n",
        "\n",
        "We are going to explore how the different variants behave in terms of the privacy cost $\\epsilon$, when the amount of noise ($\\sigma$), the sensitivity ($\\Delta f$ or $s$) and $\\delta$ have been **fixed**. For doing so, implement the functions below that calculate the sequentially composed privacy cost for each variant.\n",
        "\n",
        "Notice that for RDP, you will have to find a good value for $\\alpha$. Explore values between 1 and 100.\n",
        "\n",
        "_Hint_: Calculate first the corresponding privacy parameter for one query in terms of the parameters you have. Then, calculate the value of the sequentially composed $\\epsilon$ in the $(\\epsilon, \\delta)$ world.\n",
        "\n",
        "_Hint 2_: For the advance composition remember that the scale of noise for the gaussian distribution is: $\\sigma^2 = \\frac{2 \\Delta f^2 \\log (1.25 / \\delta)}{\\epsilon^2}$ and that the advance composition states that the total cost for k $(\\epsilon,\\delta)$ mechanisms is: $\\epsilon' = 2\\epsilon \\sqrt{2k\\log{(1 / \\delta)}}$."
      ],
      "metadata": {
        "id": "32RSyaWgETEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix sigma and delta\n",
        "sigma = 200.0\n",
        "delta = 1e-5\n",
        "sensitivity = 1\n",
        "\n",
        "# iterations\n",
        "ks = np.linspace(0, 500, 100)\n",
        "\n",
        "# Advance composition\n",
        "def seq_eps_gauss_adv(iterations, sensitivity, sigma, delta):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Renyi DP\n",
        "# You have to find an appropiate value for alpha\n",
        "alpha = None\n",
        "def seq_eps_gauss_rdp(iterations, sensitivity, sigma, alpha, delta):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "# Zero Concentrated DP\n",
        "def seq_eps_gauss_zcdp(iterations, sensitivity, sigma, delta):\n",
        "    raise NotImplementedError()\n",
        "\n",
        "ys_gauss_adv = [seq_eps_gauss_adv(k, sensitivity, sigma, delta) for k in ks]\n",
        "ys_gauss_rdp = [seq_eps_gauss_rdp(k, sensitivity, sigma, alpha, delta) for k in ks]\n",
        "ys_gauss_zcdp = [seq_eps_gauss_zcdp(k, sensitivity, sigma, delta) for k in ks]\n",
        "\n",
        "plt.plot(ks, ys_gauss_adv, label=\"Gaussian+Adv. Comp.\")\n",
        "plt.plot(ks, ys_gauss_rdp, label=\"Gaussian+RDP\")\n",
        "plt.plot(ks, ys_gauss_zcdp, label=\"Gaussian+zCDP\")\n",
        "\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.ylabel('Epsilon')\n",
        "plt.ylim(0, 3.0)\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "RObKWdVYqZ70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9 (8 points)\n",
        "\n",
        "- What is a good value for $\\alpha$ and why?\n",
        "- Does running the above code many times in order to find a good $\\alpha$ violates privacy? Why or why not?\n",
        "- What information is clear about the privacy cost behavior between the different variants?\n",
        "- How do RDP and zCDP behave in terms of the number of iterations?"
      ],
      "metadata": {
        "id": "_8Rj_xfHH93C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- YOUR ANSWER HERE\n",
        "- YOUR ANSWER HERE\n",
        "- YOUR ANSWER HERE\n",
        "- YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "TBT3ySvcLFWh"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}